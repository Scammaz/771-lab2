{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kpz6zrKDvH47"
      },
      "source": [
        "# GTI771 - Apprentissage machine avancé\n",
        "## Département de génie logiciel et des technologies de l’information\n",
        "\n",
        "\n",
        "\n",
        "## Laboratoire 5 - Régression linéaire\n",
        "#### <font color=black> Version 2 - Été 2024 </font>\n",
        "\n",
        "##### <font color=grey> Version 1 - Prof. Alessandro L. Koerich.\n",
        "##### Version 2 - Chargé de lab. Arthur Josi"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3be-ht3hvH48"
      },
      "source": [
        "| NOMS                  | CODE PERMANENT  |  PARTICIPATION     |\n",
        "|-----------------------|-----------------|--------------------|\n",
        "| Étudiant1             | Code1           |      0%            |\n",
        "| Étudiant2             | Code2           |      0%            |\n",
        "| Étudiant3             | Code3           |      0%            |"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQyLZMO_vH49"
      },
      "source": [
        "## Introduction\n",
        "Dans ce laboratoire, vous êtes amenés à utiliser des algorithmes de régression aﬁn de résoudre le problème de prédiction de l'âge de personnes à partir de photos du visage.\n",
        "\n",
        "Le problème de régression qui vous est présenté est le problème [Facial Aging Estimation (FAE)](https://yanweifu.github.io/FG_NET_data/index.html), dont le but est de prédire l'âge des personnes à partir du visage. En vous basant sur les concepts vus en classe et l'expérience acquise dans le laboratoires précedents, vous êtes invité à l’extraction de primitives puis la regression de l'âge sur l’ensemble de données fourni avec cet énoncé.\n",
        "\n",
        "##### Description de l'ensemble de données FG-NET:\n",
        "* 1002 images faciales de 82 sujets multiraciaux âgés de 0 à 69 ans;\n",
        "* Déséquilibré: 50% des sujets ont entre 0 et 13 ans;\n",
        "* Images couleur et niveaux de gris avec largeur entre 300 et 359 pixels, hauteur entre 639 et 772 pixels, et résolution entre 200 dpi et 1200 dpi;\n",
        "* Grande variation d'éclairage, de pose, d'expression faciale, de flou et d'occlusions (par exemple, moustache, barbe, lunettes, etc.).\n",
        "\n",
        "Voici, en exemple, des images de visages se retrouvant dans l’ensemble de données FG-NET:\n",
        "\n",
        "![Exemples de FG-NET](https://www.mdpi.com/sensors/sensors-16-00994/article_deploy/html/images/sensors-16-00994-g001.png)\n",
        "\n",
        "L’évaluation de ce laboratoire sera basée sur:\n",
        "- la qualité des algorithmes proposés et utilisés; (10%)\n",
        "- utilisation du protocole et mesures de performance appropriées; (10%)\n",
        "- les réponses aux questions dans ce notebook (Les cellules dans votre PDF ou votre notebook doivent être processées dans votre rendu); (70%)\n",
        "- l'organisation de votre code source (n'oubliez pas de mettre des commentaires dans le code source!) (10%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-FlZz01RvH49"
      },
      "source": [
        "# Modules et bibliotèques python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MDblbqqDvH49"
      },
      "source": [
        "### Import de bibliotèques"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SalI_s5HvH4-"
      },
      "source": [
        "###  <font color=blue> À faire: </font>\n",
        "1. Ajouter les bibliothèques que vous avez utilisées pour compléter ce notebook dans une cellule avec une petite description."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ExYpFkiWvH4-"
      },
      "outputs": [],
      "source": [
        "import numpy as np  # package for scientific computing with Python.\n",
        "import matplotlib.pyplot as plt # 2D plotting library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q8pVue-rvH4-"
      },
      "source": [
        "### Définition des fonctions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "awHsdRGzvH4_"
      },
      "outputs": [],
      "source": [
        "def fa():\n",
        "    return 1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gYk_sws2vH4_"
      },
      "source": [
        "# Partie 1 - Lecture des images et préparation des données (40%)\n",
        "\n",
        "Point de départ: 1002 images jpeg de l'ensemble FG-NET"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fc4BZxSWvH4_"
      },
      "source": [
        "## 1a: Lecture, nettoyage, prétraitement, normalisation et annotation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h4f19921vH4_"
      },
      "source": [
        "Différemment de l'ensemble FER, l’ensemble FG-NET consiste en un répertoire avec 1,002 images JPEG, où les étiquettes sont les noms des fichiers (âge et l'id du sujet).\n",
        "\n",
        "Vous devez lire ces images et les représenter sous la forme d’une matrice $X\\_data$ aussi que transformer les noms des fichiers dans un vecteur $Y\\_data$ avec les âges et un troisième vecteur $Z\\_data$ avec les id des sujets.\n",
        "\n",
        "Vous devez également, comme vous avez déjà fait pour l'ensemble de données FER, vous assurer qu’il n'y a pas:\n",
        "- données aberrantes;\n",
        "- valeurs manquantes;\n",
        "- valeurs inapplicables ou aberrantes;\n",
        "- etc.  \n",
        "PS: Pour rebalancer, ne faite pas un simple upsampling, augmentez vos données.\n",
        "\n",
        "Finalement, vous devez également appliquer de prétraitement pour réduire la variabilité, réduire des bruits, etc. En particulier, pour les images de visage, quelques prétraitements peuvent se montrer utiles, comme:\n",
        "- Localisation/recadrage du visage?\n",
        "- Localisation des yeux?\n",
        "- Lissage du visage?\n",
        "- Normalization du contraste?\n",
        "- Etc.\n",
        "\n",
        "###  <font color=blue> À faire: </font>\n",
        "\n",
        "1. Lire les images jpeg et les noms des fichiers et les représenter sous la forme de matrices de pixels et vecteurs de étiquettes ($X\\_data$, $Y\\_data$, $Z\\_data$ comme décrit précédemment)."
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "exPlBgjuzrJn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "2. Concevez et codifiez un algorithme pour vérifier l'intégrité des données, faire des corrections si nécessaires.\n"
      ],
      "metadata": {
        "id": "JAxksYrZzrnb"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HB6AXsX_zt_a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Appliquez au moins un prétraitement sur les images de visages. Vous pouvez choisir différents algorithmes de prétraitement d’images dans [scikit-image](https://scikit-image.org/docs/stable/api/api.html) ou dans la librairie de votre choix. Vous pouvez aussi trouver d’autres types de prétraitement qui sont plus généraux dans [scikit-learn](https://scikit-learn.org/stable/modules/preprocessing.html#preprocessing). Vous pouvez utiliser les mêmes prétraitements que ceux utilisés avec FER.\n"
      ],
      "metadata": {
        "id": "VfMlhql-zuyr"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hEY48r5K0ssG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Choisir et appliquer une résolution $n\\times m$ qui vous semble pertinente pour normaliser les images, car celles-ci n'ont pas toutes la même résolution."
      ],
      "metadata": {
        "id": "AUsPl-9l0tXS"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "av-52CbbzwfC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Transformez toutes les images afin que celles-ci soient toutes définies sur les 3 canaux (RGB).\n"
      ],
      "metadata": {
        "id": "3un7kK3DzxyG"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hOPvxAWhzy9A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Générez un fichier *fg-net-nxm.csv* avec les données nettoyées et normalisées, où $n$ et $m$ représentent la résolution finale des images.\n",
        "   - Format du fichier: subject,age,pixels\n",
        "      * sujet: integer\n",
        "      * âge: integer\n",
        "      * pixels: integer [0, 255]"
      ],
      "metadata": {
        "id": "KgUuiBP8z0K4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PF_U5pXCz3Qa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Décrivez brièvement les étapes de votre algorithme/code."
      ],
      "metadata": {
        "id": "pB9Pp7KEz3qQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "YemB2V3IztsP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Créer une grille de dimension 7$\\times$2 avec des images de visage prises aléatoirement de l'ensemble FG-NET original et après vos traitements. Afficher également l'âge et le id du sujet au dessus de chaque image."
      ],
      "metadata": {
        "id": "7MOkjH_A0N4K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bn5s6GeZ0Q2T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SxaU5O6vH5A"
      },
      "source": [
        "# Partie 2 - Extraction de primitives (10%)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wHr161lkvH5B"
      },
      "source": [
        "###  <font color=blue> À faire: </font>\n",
        "1. Choisir et extraire un jeu de primitives pour représenter les images de FG-NET. Vous êtes fortement conseillé de choisir\n",
        "le jeu de primitives les plus performants dans les TPs précédents (primitives artisanaux, deep, deep réduit, etc.)"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HL3jv2VW2Mg7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Sauvegardez vos vecteurs de primitives sous la forme d'un fichier 'csv' (p. ex. *fgnet-deepVGG19.csv*). N'oubliez pas d'utiliser toujours la même structure du fichier *fg-net-nxm.csv* de 1a). Vous devez nommer vos fichiers de primitives en référence au jeu de primitives utilisé, p. ex., *fg-net-12x12-deepVGG19.csv* pour des primitives produites avec une CNN VGG19."
      ],
      "metadata": {
        "id": "HuCYpcTx2OH3"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kg41QnGA2YiY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Décrivez très brièvement le choix du jeu de primitives."
      ],
      "metadata": {
        "id": "Nbl7ffgk2ZDG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l7dBD3_N2aUM"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hCMLmVIFvH5B"
      },
      "source": [
        "# Partie 3: Entraînement de modèles de régression (20%)\n",
        "\n",
        "Vous êtes maintenant prêtes à entraîner un modèle d'apprentissage automatique avec les vecteurs de primitives extraits dans la Partie 2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ndJu9L0mvH5B"
      },
      "source": [
        "###  <font color=blue> À faire: </font>\n",
        "1. Choisir deux (2) algorithmes de régression disponibles dans Scikit-learn:\n",
        "    * Régression lineaire\n",
        "    * Régression Ridge\n",
        "    * Régression Lasso et Elastic-Net\n",
        "    * Descente du gradiente stochastique (SGD)<br>\n",
        "\n",
        "Conseil: Étudiez les algorithmes choisis pour bien comprendre les différents hyperparamètres qui peuvent affecter l'entraînement, la généralisation et la complexité du modèle de régression."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Algorithmes choisis: \\<ici\\>"
      ],
      "metadata": {
        "id": "pU3jIrFB2hse"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Entraîner et optimiser les paramètres des modèles de régression. Utiliser le protocole <font color=blue> \"Leave One Subject Out Cross-Validation\" </font> (LOSO)."
      ],
      "metadata": {
        "id": "EXmO4CaB3Pc9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "H2zJGXWo3P4k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Étudiez les metriques d'évaluation des modèles de régression disponibles dans [Scikit-Learn](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics) puis utilisez MSE, MAE et une troisième metrique de votre choix.\n"
      ],
      "metadata": {
        "id": "ZVSLgaiW3J4g"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "n5Izilfm3JGV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Faire une brève analyse des résultats et présenter vos considérations et conclusions sur les algorithmes de régression choisis."
      ],
      "metadata": {
        "id": "ZpEHwefk3Uwn"
      }
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "L05fk3xh5c7g"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "| Algorithme            | Paramètres    |  MSE  |  MAE  | \\<votre choix\\> |\n",
        "|-----------------------|---------------|-------|-------|-------|\n",
        "| Regr lineaire         | XXX.XX        |XXX.XX |XXX.XX |       |\n",
        "| Regr Ridge            | alpha = 0.1   |123.34 | 10.45 |       |\n",
        "| Regr Lasso            | XXX.XX        |XXX.XX |XXX.XX |       |\n",
        "| Regr ElasticNet       | XXX.XX        |XXX.XX |XXX.XX |       |\n",
        "| ...                   | XXX.XX        |XXX.XX |XXX.XX |       |\n",
        "| ...                   | XXX.XX        |XXX.XX |XXX.XX |       |"
      ],
      "metadata": {
        "id": "okCDeACJ3sm4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7Ng9NzjcvH5C"
      },
      "source": [
        "# Fin"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.8"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}